---
title: Comparison of Some Commonly Used RL Algorithms
subtitle: "åŒ…æ‹¬ PPO, RLOO, GRPO, REINFORCE++ï¼Œå’Œä¸€äº›ä¼ªä»£ç å®žçŽ°"
date: 2025-03-14 19:16:43
tags: [dl, rl]
series: 1
---

# Comparison of Some Commonly Used RL Algorithms

ä»Ž deepseek ðŸ³ å…¬å¸ƒ [r1 technical report](https://arxiv.org/abs/2501.12948) åŽï¼Œrl åœ¨ reasoning æ–¹é¢çš„åº”ç”¨é€æ¸å—åˆ°å…³æ³¨ã€‚prm å·²ç»æ˜¯è¿‡åŽ»å¼ï¼Œrule based reward æ‰æ˜¯å”¯ä¸€çš„å‡ºè·¯ã€‚ æ‰€ä»¥è¯´ï¼Œè¦æƒ³ä¸è½åŽäºŽæ—¶ä»£ï¼Œäººäººéƒ½éœ€è¦æ‡‚å¾—ä¸€ç‚¹ rlã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œåšä¸»å°†æ¯”è¾ƒä¸€äº›åœ¨reasoningè®­ç»ƒä¸­å¸¸ç”¨çš„rlç®—æ³•ï¼ŒåŒ…æ‹¬PPO, RLOO, GRPO, REINFORCE++ã€‚

è€Œä¸”å¯¹åšä¸»æœ¬äººæ¥è¯´ï¼Œä»£ç æ¯”å…¬å¼æ›´å¥½æ‡‚ï¼Œæ‰€ä»¥æˆ‘ä¼šç”¨ä¼ªä»£ç æ¥è§£é‡Šè¿™äº›ç®—æ³•ã€‚ðŸ˜Ž
> ä¼ªä»£ç å®žçŽ°ä¸»è¦å‚è€ƒ OpenRLHF å’Œ VeRl

## Backgroundï¼šåœ¨ LLM é‡Œæˆ‘ä»¬æ€Žä¹ˆä½¿ç”¨ RL

å…¶å®žä¸ç®¡ä»€ä¹ˆç®—æ³•ï¼Œåœ¨rlé‡Œä½¿ç”¨çš„åŸºæœ¬æ­¥éª¤éƒ½æ˜¯
1. generate experience
   
   æˆ‘ä»¬ç»´æŠ¤ä¸€ä¸ªexperience replay bufferã€‚åœ¨æ¯ä¸ªstepï¼Œé¦–å…ˆåšactorç”¨é‡‡æ ·ï¼Œå¾—åˆ°policy model å’Œ ref modelå¯¹åº”çš„logitsï¼Œç„¶åŽç”¨reward modelæ‰“åˆ†å¾—åˆ°rewardï¼Œæœ€åŽä¼°è®¡advantage
2. è®­ç»ƒpolicy model
   
   ç”¨ä¼°è®¡çš„advantageæ¥æ›´æ–°policy model
3. (optional) æ›´æ–°critic model
   
   ç”¨td erroræ¥æ›´æ–°critic model

## Comparison

å…¶å®žè¿™å‡ ä¸ªæ–¹æ³•éƒ½åœ¨è®­ç»ƒçš„æ—¶å€™ç”¨äº† ppo-clipï¼Œæ‰€ä»¥åŒºåˆ«å…¶å®žåœ¨generate experienceçš„æ—¶å€™ã€‚

## Advantage Estimation

### PPO

PPOç”¨GAEä¼°è®¡advantage ï¼ˆä½ å¯ä»¥åœ¨[PPO blog](https://linn3a.github.io/blog/posts/dl/GAE)ä¸­æ‰¾åˆ°æˆ‘å¯¹GAEçš„è§£é‡Šï¼‰

ç®€å•æ¥è¯´ GAEæ˜¯ä¸€ä¸ªå¯¹ $\delta$(TD residual)çš„åŠ æƒå’Œï¼Œæƒé‡æ˜¯$\lambda$å’Œ$\gamma$ï¼Œ$\lambda$æ˜¯åŠ æƒæ±‚å’Œçš„æƒé‡ï¼Œ$\gamma$æ˜¯ td-k ä¸­çš„ discount factor

å½“ç„¶ï¼Œå› ä¸º $\delta$ ç”¨åˆ°äº† valueï¼Œæ‰€ä»¥åœ¨ppoä¸­æˆ‘ä»¬éœ€è¦ç”¨åˆ°ä¸€ä¸ª critic model

ä¼ªä»£ç å®žçŽ°å¦‚ä¸‹ï¼š

```python
# 1. scalar reward => reward of each step

# compute kl
kl = log_prob - log_prob_ref

reward = torch.zeros_like(kl)
reward[eos_index] = scalar_reward
reward = reward - kl_penalty * kl

# 2. compute advantage

# get value
value = critic(sequences, action_mask)

# gae

# delta_i = r_i + gamma * V(s_{i+1}) - V(s_i) 
# A(s_i, a_i) = delta_i + gamma * lambda * delta_{i+1} + gamma^2 * lambda^2 * delta_{i+2} + ...

# compute from end of sequence
last_item = 0
last_value = 0
advantages_reversed = []

for t in reversed(range(response_length)):
    nextvalues = values[:, t + 1] if t < response_length - 1 else 0.0
    delta = rewards[:, t] + gamma * nextvalues - values[:, t]
    last_item = delta + gamma * lambda_ * last_item
    advantages_reversed.append(last_item)
advantages = torch.stack(advantages_reversed[::-1], dim=1)
returns = advantages + values
```

### RLOO

éžå¸¸å¥½çš„RLOOè®²è§£å¯ä»¥åœ¨ [ðŸ¤—](https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo) è¿™é‡Œæ‰¾åˆ°

åœ¨GAEçš„åŽŸè®ºæ–‡é‡Œå…¶å®žæåˆ°ï¼Œæœ‰å¾ˆå¤šç§ä¼°è®¡advantageçš„æ–¹æ³•ï¼Œé™¤äº†ç”¨ critic model ä¼°è®¡ valueï¼Œè¿˜å¯ä»¥ç”¨ ç»„é—´çš„ baseline æ›¿ä»£ value

RLOO çš„ baseline è®¾ç½®æ˜¯ leave one outï¼Œå³ç”¨ç»„å†…é™¤äº†è¯¥æ ·æœ¬ä¹‹å¤–çš„æ‰€æœ‰æ ·æœ¬çš„å¹³å‡å€¼ä½œä¸ºbaseline

![alt text](/blog/images/compare_rl/image.png)

ä»£ç å®žçŽ°ä¸Šè¯·å‚è€ƒ

```python
# 1. scalar reward => reward of each step

# compute kl
kl = log_prob - log_prob_ref

rewards = torch.zeros_like(kl)
rewards[:,eos_index] = scalar_reward
reward = reward - kl_penalty * kl

# 2. compute rloo reward

rewards = torch.cat(rewards)
rewards = rewards.reshape(-1, args.n_samples_per_prompt).to(device="cuda")
baseline = (rewards.sum(-1, keepdim=True) - rewards) / (args.n_samples_per_prompt - 1)
rewards = rewards - baseline
rewards = rewards.flatten().chunk(len(experiences))

# 3. convert to advantage
# use accumulated reward 
accumulated_return = torch.zeros(reward.size(0))
returns = torch.zeros_like(reward)

for t in reversed(range(reponse_length)):
    accumulated_reward = reward[:,t] + gamma * accumulated_reward
    returns[:, t] = accumulated_reward 
```


> [!note] è¡¥å……
>
> è™½ç„¶åŽŸè®ºæ–‡é‡Œè¯´RLOOæ˜¯æŠŠæ•´ä¸ªsequenceè§†ä½œä¸€ä¸ªactionï¼Œä½†æ˜¯ä»Žå„ä¸ªæ¡†æž¶çš„ä»£ç å®žçŽ°æ¥è¯´ï¼Œè¿˜æ˜¯ä¼šæŠŠadvantagåˆ†åˆ°æ¯ä¸ªtokenä¸Šï¼Œè¿™æ ·åœ¨ç®—lossçš„æ—¶å€™å°±æ²¿ç”¨policy gradientï¼ŒæŠŠæ¯ä¸ªtokençš„advantage å’Œ logpä¹˜èµ·æ¥

### GRPO

GRPO æ˜¯ç”± Deepseek æå‡ºçš„ä¸€ç§ RL ç®—æ³•ï¼Œå®ƒå’Œ RLOO ä¸€æ ·ï¼Œç”¨ baseline å–ä»£äº† critic model

[DeepseekMath](https://arxiv.org/abs/2402.03300) æ¯”è¾ƒäº† GRPO å’Œ PPO çš„ç®—æ³•åŒºåˆ«ï¼š

![alt text](/blog/images/compare_rl/image-1.png)

GRPO ç”¨ group normalized reward ä½œä¸º baselineï¼Œreward - baseline å°±æ˜¯ advantage

å¹¶ä¸”åœ¨ç®— loss ä¸­çš„ kl penaltyæ—¶ï¼ŒGRPO ç”¨ unbiased kl estimator è®¡ç®— klï¼Œå³ $r - 1 - logr$, å…¶ä¸­ $r=\log\frac{p_{ref}}{p}$

ä¼ªä»£ç å®žçŽ°å¦‚ä¸‹ï¼Œå…¶å®žå’ŒRLOOå·®ä¸å¤šï¼š

```python
# 1. scalar reward => reward of each step

# compute kl
kl = log_prob - log_prob_ref

rewards = torch.zeros_like(kl)
rewards[;, eos_index] = scalar_reward
reward = reward - kl_penalty * kl

# 2. compute group normalized reward

rewards = rewards.reshape(-1, args.n_samples_per_prompt)
rewards = (rewards - rewards.mean(-1, keepdim=True)) / (rewards.std(-1, keepdim=True) + 1e-9)
rewards = rewards.reshape(-1).chunk(len(experiences))

# 3. convert to advantage
# use accumulated reward 
accumulated_return = torch.zeros(reward.size(0))
returns = torch.zeros_like(reward)

for t in reversed(range(reponse_length)):
    accumulated_reward = reward[:,t] + gamma * accumulated_reward
    returns[:, t] = accumulated_reward 
```

### REINFORCE++

å’Œ grpo æ›´åƒï¼Œåªæ˜¯æ²¡æœ‰ç»„çš„æ¦‚å¿µï¼Œç›´æŽ¥å¯¹æ•´ä¸ª batch åš normalization

```python
# 1. scalar reward => reward of each step

# compute kl
kl = log_prob - log_prob_ref

rewards = torch.zeros_like(kl)
rewards[;, eos_index] = scalar_reward
reward = reward - kl_penalty * kl

# 2. compute group normalized reward
# list => tensor
rewards = torch.cat(rewards)
rewards = (rewards - rewards.mean(-1, keepdim=True)) / (rewards.std(-1, keepdim=True) + 1e-9)
# tensor => list
rewards = rewards.reshape(-1).chunk(len(experiences))

# 3. convert to advantage
# use accumulated reward 
accumulated_return = torch.zeros(reward.size(0))
returns = torch.zeros_like(reward)

for t in reversed(range(reponse_length)):
    accumulated_reward = reward[:,t] + gamma * accumulated_reward
    returns[:, t] = accumulated_reward 
```

