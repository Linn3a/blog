---
title: Jax 101-1 ğŸ¤
subtitle: My note of learning Jax
date: 2024-01-26 00:33:00
tags: [dl,jax]
series: 5
cover: /images/jax.jpg
---

> [!abstract] Abstract
>
> å¯ä»¥çœ‹ä½œå®˜æ–¹æ–‡æ¡£[^1]çš„ç¿»è¯‘å’Œç®€åŒ–ç‰ˆ

# JAX As Accelerated NumPy

## np => jnp

æŠŠ`np`å˜æˆ`jnp` å¯ä»¥ä»numpyæ— ç¼åˆ‡æ¢

```python
import jax
import jax.numpy as jnp

x = jnp.arange(10)
```

```
[0 1 2 3 4 5 6 7 8 9]
```

## jax.grad

`jax.grad`æ˜¯jaxæä¾›çš„å¾®åˆ†æ–¹æ³•ï¼Œä¸åŒäºpytorchçš„`loss.backward()`ï¼Œjaxçš„æ¢¯åº¦æ˜¯æ‰‹åŠ¨è®¡ç®—çš„ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ã€‚

`jax.grad`æ¥å—å‚æ•°ä¸ºå‡½æ•°ï¼Œè¾“å‡ºç»“æœä¹Ÿæ˜¯å‡½æ•°

- é»˜è®¤æƒ…å†µ

```python
grad_fn = jax.grad(f)
grad_fn(x,y)
```

`grad_fn(x,y)`çš„ç»“æœæ˜¯`f(x,y)`å¯¹ç¬¬ä¸€ä¸ªå‚æ•°`x`çš„æ¢¯åº¦

- æŒ‡å®šå¾®å…ƒ

```python
grad_fn = jax.grad(f, argnums=(0, 1))
```

è¿”å›å€¼æ˜¯ä¸€ä¸ªäºŒå…ƒå…ƒç»„ï¼ŒåŒ…å«æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦

- åœ¨==trainning==ä¸­

gradå¯ä»¥å¤„ç†`a nested dict of arrays`ï¼Œæ‰€ä»¥åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå¯ä»¥ç›´æ¥ï¼š

```python
grads = jax.grad(loss_fn)(params, data_batch)
```

### Value and Grad

`jax.grad`åªè¿”å›æ¢¯åº¦å€¼ï¼Œä½†æ˜¯ä¸€äº›æƒ…å†µä¸‹ï¼Œé™¤äº†æ¢¯åº¦è¿˜éœ€è¦çŸ¥é“å‡½æ•°çš„å€¼ï¼Œæ¯”å¦‚trainingè¿‡ç¨‹ä¸­å¯èƒ½éœ€è¦è®°å½•lossçš„å€¼

```python
jax.value_and_grad(f)(x, y)
```

è¿”å›äºŒå…ƒå…ƒç»„ï¼Œ(value, $\frac{\partial f}{\partial x}$)

> éƒ½æ˜¯jnp

### Auxiliary data

å¯¹å¤šä¸ªè¿”å›å€¼çš„`f`ï¼Œset attribute: `has_aux=True`

```python
jax.grad(f, has_aux=True)(x, y)
```

##  No side-effects

Jaxçš„è®¾è®¡å®—æ—¨ï¼Œä¸è¦å†™æœ‰side-effectçš„ä»£ç 

> A side-effect is any effect of a function that doesnâ€™t appear in its output.
>
> Side-effect-free code is sometimes called *functionally pure*, or just *pure*.

æ¯”å¦‚

```python
import numpy as np

x = np.array([1, 2, 3])

def in_place_modify(x):
  x[0] = 123
  return None

in_place_modify(x)
x
```

åœ¨numpyä¸­å¯ä»¥è¿™æ ·å†™ï¼Œä½†æ˜¯åœ¨jaxä¸­ä¼šæŠ¥é”™

åœ¨jaxä¸­ï¼Œéœ€è¦

```python
def jax_in_place_modify(x):
  return x.at[0].set(123)

y = jnp.array([1, 2, 3])
jax_in_place_modify(y)
```

ä¸æ˜¯æ”¹å˜åŸç»“æ„ï¼Œè€Œæ˜¯è¿”å›ä¸€ä¸ªå‰¯æœ¬ï¼Œå‚æ•°æ˜¯ä¿æŒä¸å˜çš„

## A training loop

åˆ›å»ºä¸€äº›å‡æ•°æ®ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt

xs = np.random.normal(size=(100,))
noise = np.random.normal(scale=0.1, size=(100,))
ys = xs * 3 - 1 + noise

plt.scatter(xs, ys);
```

å®šä¹‰ä¸€ä¸ªæœ€ç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œä¸€ä¸ªl2-distçš„æŸå¤±å‡½æ•°ï¼Œä¸€ä¸ªä¼˜åŒ–å™¨

```python
def model(theta, x):
    w,b = theta
    return w*x + b

def loss_fn(theta, x, y):
    prediction = model(theta, x)
    return jnp.mean((prediction - y)** 2)

def update(theta, x, y, lr=0.1):
    return theta - lr * jax.grad(loss_fn)(theta, x, y)
```

training loop:

```python
theta = jnp.array([1., 1.])

for _ in range(1000):
    theta = update(theta, xs, ys)

plt.scatter(xs, ys)
plt.plot(xs, model(theta, xs))

w, b = theta
```



[^1]:https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html
