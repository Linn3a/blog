---
title: Jax 101-1 🤞
subtitle: My note of learning Jax
date: 2024-01-26 00:33:00
tags: [dl,jax]
series: 5
cover: /images/jax.jpg
---

> [!abstract] Abstract
>
> 可以看作官方文档[^1]的翻译和简化版

# JAX As Accelerated NumPy

## np => jnp

把`np`变成`jnp` 可以从numpy无缝切换

```python
import jax
import jax.numpy as jnp

x = jnp.arange(10)
```

```
[0 1 2 3 4 5 6 7 8 9]
```

## jax.grad

`jax.grad`是jax提供的微分方法，不同于pytorch的`loss.backward()`，jax的梯度是手动计算的，使用方法如下。

`jax.grad`接受参数为函数，输出结果也是函数

- 默认情况

```python
grad_fn = jax.grad(f)
grad_fn(x,y)
```

`grad_fn(x,y)`的结果是`f(x,y)`对第一个参数`x`的梯度

- 指定微元

```python
grad_fn = jax.grad(f, argnums=(0, 1))
```

返回值是一个二元元组，包含每个参数的梯度

- 在==trainning==中

grad可以处理`a nested dict of arrays`，所以在深度学习中，可以直接：

```python
grads = jax.grad(loss_fn)(params, data_batch)
```

### Value and Grad

`jax.grad`只返回梯度值，但是一些情况下，除了梯度还需要知道函数的值，比如training过程中可能需要记录loss的值

```python
jax.value_and_grad(f)(x, y)
```

返回二元元组，(value, $\frac{\partial f}{\partial x}$)

> 都是jnp

### Auxiliary data

对多个返回值的`f`，set attribute: `has_aux=True`

```python
jax.grad(f, has_aux=True)(x, y)
```

##  No side-effects

Jax的设计宗旨，不要写有side-effect的代码

> A side-effect is any effect of a function that doesn’t appear in its output.
>
> Side-effect-free code is sometimes called *functionally pure*, or just *pure*.

比如

```python
import numpy as np

x = np.array([1, 2, 3])

def in_place_modify(x):
  x[0] = 123
  return None

in_place_modify(x)
x
```

在numpy中可以这样写，但是在jax中会报错

在jax中，需要

```python
def jax_in_place_modify(x):
  return x.at[0].set(123)

y = jnp.array([1, 2, 3])
jax_in_place_modify(y)
```

不是改变原结构，而是返回一个副本，参数是保持不变的

## A training loop

创建一些假数据：

```python
import numpy as np
import matplotlib.pyplot as plt

xs = np.random.normal(size=(100,))
noise = np.random.normal(scale=0.1, size=(100,))
ys = xs * 3 - 1 + noise

plt.scatter(xs, ys);
```

定义一个最简单的线性回归模型，一个l2-dist的损失函数，一个优化器

```python
def model(theta, x):
    w,b = theta
    return w*x + b

def loss_fn(theta, x, y):
    prediction = model(theta, x)
    return jnp.mean((prediction - y)** 2)

def update(theta, x, y, lr=0.1):
    return theta - lr * jax.grad(loss_fn)(theta, x, y)
```

training loop:

```python
theta = jnp.array([1., 1.])

for _ in range(1000):
    theta = update(theta, xs, ys)

plt.scatter(xs, ys)
plt.plot(xs, model(theta, xs))

w, b = theta
```



[^1]:https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html
