---
title: ğŸ‘†æ–‡è¯¦è§£PPO 
date: 2024-10-18 16:26:43
tags: [dl,rl]
series: 1
cover: /blog/images/ppo_cover.png
---
# ğŸ‘†æ–‡è¯¦è§£PPO

## 01. é‡è¦æ€§é‡‡æ ·ï¼ˆImportance Samplingï¼‰

ä¹‹å‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¸¸å¸¸é‡‡ç”¨on-policyæ–¹æ³•ï¼Œä¸ºäº†ä¿è¯å½“å‰å­¦ä¹ çš„agentå’Œæ¢ç´¢çš„agentä¸€è‡´ï¼Œé€šå¸¸è¦åœ¨æ¯æ¬¡æ›´æ–°å‚æ•°åç”¨æ–°çš„agentå†æ¬¡æ¢ç´¢ç¯å¢ƒï¼Œä¿å­˜æ•°æ®ï¼Œä»¥è¿›è¡Œä¸‹ä¸€è½®å‚æ•°æ›´æ–°ã€‚
ä¸ä¹‹ç›¸å¯¹çš„ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨off-policyæ–¹æ³•æ¥å­¦ä¹ ç­–ç•¥ã€‚æ˜¾è‘—çš„å¥½å¤„æ˜¯ï¼Œå¯ä»¥ç”¨åˆå§‹çš„agentæ¢ç´¢ç¯å¢ƒï¼Œå°½é‡**ä¸€æ¬¡æ€§**æ”¶é›†è¶³å¤Ÿå¤šçš„äº¤äº’æ•°æ®ç”¨äºå­¦ä¹ ã€‚
é‚£ä¹ˆï¼Œå¦‚ä½•ä¿è¯æ—§çš„äº¤äº’æ•°æ®å¯ä»¥æœåŠ¡äºæ–°çš„agentå‘¢ï¼Ÿè¿™é‡Œéœ€è¦ç”¨åˆ°**é‡è¦æ€§é‡‡æ ·**çš„æ€æƒ³ã€‚
### é‡è¦æ€§é‡‡æ ·

å‡è®¾æœ‰åˆ†å¸ƒ $p$, $q$ï¼Œæœ‰æœŸæœ›

$$\mathbb E_{x\sim p}[f(x)]=\int f(x)p(x)dx=\int f(x)\frac{p(x)}{q(x)}q(x)\\=\mathbb E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]$$

æ‰€ä»¥è¯´ï¼Œåªè¦ä¹˜é‡è¦æ€§æƒé‡$$\frac{p(x)}{q(x)}$$
å°±å¯ä»¥ç”¨å¦ä¸€ä¸ªåˆ†å¸ƒçš„æ•°æ®è®¡ç®—å½“å‰åˆ†å¸ƒä¸‹çš„æœŸæœ›

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¹˜ä¸Šé‡è¦æ€§æƒé‡åï¼Œä¸¤ä¸ªåˆ†å¸ƒä¸‹çš„æœŸæœ›ä¿æŒä¸å˜ï¼Œä½†æ˜¯æ–¹å·®ä¸åŒï¼Œæœ‰ï¼š
$$\mathbb{var}_{x\sim p}[f(x)]=\mathbb E_{x\sim p}[f(x)^2]-(\mathbb E_{x\sim p}[f(x)])^2$$

$$\mathbb{var}_{x\sim q}[f(x)\frac{p(x)}{q(x)}]=\mathbb E_{x\sim q}[(f(x)\frac{p(x)}{q(x)})^2]-(\mathbb E_{x\sim q}[f(x)\frac{p(x)}{q(x)}])^2\\=\mathbb E_{x\sim p}[f(x)^2\frac{p(x)}{q(x)}]-(\mathbb E_{x\sim p}[f(x)])^2$$

ä¸¤ä¸ªåå·®çš„ç¬¬ä¸€é¡¹æ˜¯ä¸åŒçš„ æ‰€ä»¥å¦‚æœé‡‡æ ·æ¬¡æ•°è¿‡å°‘ï¼Œç”±äºæ–¹å·®æœ‰å·®åˆ«ï¼Œæ ¹æ®æ”¶é›†åˆ°çš„æ•°æ®è®¡ç®—çš„æœŸæœ›å¯èƒ½å’ŒçœŸå®æœŸæœ›ä¸ä¸€è‡´

### æ€ä¹ˆæŠŠé‡è¦æ€§é‡‡æ ·è¿ç§»åˆ°å¼ºåŒ–å­¦ä¹ ï¼Ÿ

ç­–ç•¥æ¢¯åº¦ï¼š
$$\nabla \overline R_{\theta}=\mathbb E_{\tau \sim p_{\theta}(\tau)}[R(\tau)\nabla\log p_{\theta}(\tau)]$$

å¦‚æœåŠ ä¸Šé‡è¦æ€§é‡‡æ ·ï¼Œå‚è€ƒçš„agentå‚æ•°ä¸º$\theta'$
æœ‰ï¼š
$$\nabla \overline R_{\theta}=\mathbb E_{\tau \sim p_{\thetaâ€™}(\tau)}[R(\tau)\frac{p_{\theta}(x)}{p_{\theta'}(x)}\nabla\log p_{\theta}(\tau)]$$

> [!info] é‡è¦æ€§æƒé‡
> é‡è¦æ€§æƒé‡æ˜¯æ¦‚ç‡åˆ†å¸ƒçš„æ¯”å€¼ï¼Œå’Œå‰é¢æœŸæœ›çš„è§’æ ‡æ˜¯å¯¹åº”çš„
> æ‰€ä»¥è¦ä¹˜$\frac{p_{\theta}(x)}{p_{\theta'}(x)}$

å®è·µä¸­ï¼Œé€šå¸¸ä¸æ˜¯ç”¨æ•´ä¸ªtrajectoryï¼Œè€Œæ˜¯ç”¨æ¯ä¸€ä¸ªstate-actionå¯¹æ¥æ›´æ–°æƒé‡ï¼Œå³
$$\mathbb E_{(s_t,a_t)\sim \pi_{\theta}}[A^{\theta}(s_t,a_t)\nabla\log p_\theta(a_t|s_t)]$$
ç”¨é‡è¦æ€§é‡‡æ ·ï¼š
$$\mathbb E_{(s_t,a_t)\sim \pi_{\theta'}}[\frac{p_\theta(s_t,a_t)}{p_\theta'(s_t,a_t)}A^{\theta}(s_t,a_t)\nabla\log p_\theta(a_t|s_t)]$$

å› ä¸º$P_\theta(a_t|a_t)$ å°±æ˜¯policyçš„è¾“å‡º
æ‰€ä»¥ç”¨è´å¶æ–¯
$P(s_t,a_t) = p(a_t|s_t)p(s_t)$
æ‰€ä»¥ï¼š
$$\mathbb E_{(s_t,a_t)\sim \pi_{\theta'}}[\frac{p_\theta(a_t|s_t)p_\theta(s_t)}{p_{\theta'}(a_t|s_t)p_{\theta'}(s_t)}A^{\theta}(s_t,a_t)\nabla\log p_\theta(a_t|s_t)]$$

å› ä¸ºï¼š
- çŠ¶æ€é€šå¸¸å’ŒåŠ¨ä½œæ— å…³ï¼Œå³ä¸ºä¸å‚æ•°æ— å…³
- çŠ¶æ€çš„æ¦‚ç‡éš¾ä»¥ä¼°è®¡
æ‰€ä»¥å‡è®¾ $p_\theta(s_t) = p_{\theta'}(s_t)$ï¼Œçœå»è¿™ä¸€é¡¹
å¾—åˆ°ï¼š
$$\mathbb E_{(s_t,a_t)\sim \pi_{\theta'}}[\frac{p_\theta(a_t|s_t)}{p_{\theta'}(a_t|s_t)}A^{\theta}(s_t,a_t)\nabla\log p_\theta(a_t|s_t)]$$

ç”¨æ¢¯åº¦åæ¨ç›®æ ‡å‡½æ•°ï¼Œç”±äº
$$\nabla f(x)=f(x)\nabla\log f(x)$$
æŠŠ $\theta'$ ç›¸å…³çš„éƒ½çœ‹ä½œå¸¸æ•°ï¼Œå¾—åˆ°ç›®æ ‡å‡½æ•°ï¼š
$$J_{\theta'}(\theta) = \mathbb E_{(s_t,a_t)\sim \pi_{\theta'}}[\frac{p_\theta(a_t|s_t)}{p_{\theta'}(a_t|s_t)}A^{\theta}(s_t,a_t))$$
## 02. è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰

å‰é¢è¯´è¿‡äº†ï¼Œä¹˜ä¸Šé‡è¦æ€§æƒé‡ååŸæ¥çš„åˆ†å¸ƒå¹¶ä¸æ˜¯å®Œå…¨ç­‰åŒçš„ã€‚å¦‚æœä¸¤ä¸ªåˆ†å¸ƒç›¸å·®è¿‡å¤§ï¼Œæˆ–è€…æ–¹å·®è¿‡å¤§ï¼Œé‡è¦æ€§é‡‡æ ·çš„æ•ˆæœå°±ä¸ä¼šå¾ˆå¥½
ppoçš„ä¸»è¦æ€æƒ³å°±æ˜¯å¢åŠ ä¸€ä¸ªçº¦æŸï¼Œä»¥ä½¿å¾—ä¸¤ä¸ªåˆ†å¸ƒè·ç¦»ä¸å¤§ï¼Œå¾ˆå®¹æ˜“æƒ³åˆ°è¿™é‡Œå¢åŠ çš„åˆ†å¸ƒæ˜¯KLæ•£åº¦

> [!info] KLæ•£åº¦
> KLæ•£åº¦çš„ç‰©ç†æ„ä¹‰ï¼šé¢„æµ‹åˆ†å¸ƒ ä¸ çœŸå®åˆ†å¸ƒçš„è·ç¦»
> 
> $$D(p||q)=H(p,q)-H(p)\\=-\sum p(x)\log(q(x)) + \sum p(x)\log (p(x))\\=-\sum p(x)\log \frac{q(x)}{p(x)}$$
> 
> è¿™é‡Œç”¨klæ•£åº¦ è€Œä¸æ˜¯ç›´æ¥å¯¹å‚æ•°æ±‚æ­£åˆ™ åŸå› æ˜¯è¡¡é‡çš„æ˜¯actionçš„è·ç¦»ï¼Œè€Œä¸æ˜¯å‚æ•°çš„è·ç¦»

åŠ ä¸Šçº¦æŸä¹‹åï¼š
$$J_{PPO}^{\theta'}=J^{\theta'}(\theta)-\beta KL(\theta,\theta')$$

$$J_{\theta'}(\theta) = \mathbb E_{(s_t,a_t)\sim \pi_{\theta'}}[\frac{p_\theta(a_t|s_t)}{p_{\theta'}(a_t|s_t)}A^{\theta}(s_t,a_t)]$$

æ€è·¯ç›¸åŒçš„è¿˜æœ‰ä¸€ç¯‡æ–‡ç« ï¼šä¿¡ä»»åŒºåŸŸç­–ç•¥ä¼˜åŒ–ï¼ˆtrust region policy optimization, TRPOï¼‰

$$J_{TRPO}^{\theta'}=\mathbb E_{(s_t,a_t)\sim \pi_{\theta'}}[\frac{p_\theta(a_t|s_t)}{p_{\theta'}(a_t|s_t)}A^{\theta}(s_t,a_t)],~KL(\theta,\theta')<\delta$$

ä½†æ˜¯è¿™ç§çº¦æŸåœ¨åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•ä¸­æ˜¯å¾ˆéš¾å¤„ç†çš„

PPOæœ‰ä¸¤ç§å˜ç§ï¼Œåˆ†åˆ«æ˜¯åŸºäºæƒ©ç½šé¡¹çš„ PPO-Penalty å’ŒåŸºäºè£å‰ªçš„ PPO-Clip
### PPO-Penalty
å³ï¼š

$$J_{PPO1}^{\theta^k}=J^{\theta^k}(\theta)-\beta KL(\theta,\theta^k)$$

$$J_{\theta^k}(\theta) \approx \sum\limits_{(s_t,a_t)}\frac{p_\theta(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}A^{\theta}(s_t,a_t)$$

åŸè®ºæ–‡ä¸­ $\beta$ å€¼æ˜¯å¯ä»¥ä¿®æ”¹çš„ ä¹Ÿç§°ä¸ºadaptive PPO penalty

è‹¥$KL>KL_{max}$  å°±è¯´æ˜åé¢è¿™é¡¹æ²¡æœ‰é€ æˆä»€ä¹ˆçº¦æŸ å¢å¤§ $\beta$

è‹¥$KL<KL_{max}$  å°±è¯´æ˜åé¢è¿™é¡¹æ²¡æœ‰é€ æˆçš„çº¦æŸè¿‡å¤§ å‡å° $\beta$
### PPO-Clip
ä¸å¼•å…¥kLæ•£åº¦ï¼Œè€Œæ˜¯ç›´æ¥çº¦æŸé‡è¦æ€§æƒé‡çš„å€¼
è£å‰ªé‡è¦æ€§æƒé‡ï¼Œä½¿å¾—
$$1-\epsilon \le \frac{p_{\theta}(a_t|s_t)}{p_{\theta'}(a_t|s_t)}\le 1+\epsilon$$
æ‰€ä»¥ï¼š

$$J_{PPO2}^{\theta^k}=\sum\limits_{(s_t,a_t)}\min(\frac{p_\theta(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}A^{\theta}(s_t,a_t),clip(\frac{p_\theta(a_t|s_t)}{p_{\theta^k}(a_t|s_t)},1-\epsilon,1+\epsilon)A^{\theta}(s_t,a_t))$$
