---
title: Jax 101-4 🤞
subtitle: Advanced Automatic Differentiation in Jax 🎂
date: 2024-01-29 01:20:00
tags: [dl,jax]
series: 5
cover: blog/images/jax.jpg
---

> [!abstract] Abstract
>
> 简单但重要的一节 🫣

# Advanced Automatic Differentiation in Jax 🎂

## Higher-order derivatives

求高阶导数在Jax中是容易的

例如对$f(x)=x^3+2x^2-3x+1$，可以表示为：

```python
import jax
f = lambda x: x**3 + 2*x**2 - 3*x + 1
dfdx = jax.grad(f)
```

重复使用`jax.grad`得到多阶导数

```python
d2fdx = jax.grad(dfdx)
d3fdx = jax.grad(d2fdx)
d4fdx = jax.grad(d3fdx)
```

有多个参数时，梯度值是Hessian矩阵。Jax提供了`jax.jaxfwd`和`jax.jaxrec`两个方法来计算梯度值，分别对应forward- and reverse-mode autodiff

> [!note] Note
>
> `jax.jaxfwd`和`jax.jaxrec`的结果完全一致，只是适用场景不同，有效率区别
>
> 官方提供的自动微分视频[^1]

> [!info] More information
>
> 更多关于梯度和hassian矩阵的优化和使用方法在[The Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)

## Higher order optimization

在一些metaa-learning方法中（如Model-Agnostic Meta-Learning ([MAML](https://arxiv.org/abs/1703.03400)),）需要对梯度更新过程做微分，相当于对特征值做多阶导，在Jax中可以轻松实现

```python
def meta_loss_fn(params, data):
  """Computes the loss after one step of SGD."""
  grads = jax.grad(loss_fn)(params, data)
  return loss_fn(params - lr * grads, data)

meta_grads = jax.grad(meta_loss_fn)(params, data)
```

## Stopping gradients

用`jax.lax.stop_gradient(f)`表示忽略这一部分的梯度（假装其不依赖于参数x）

官方文档这里举了一个RL的例子[^2]

### Straight-through estimator using `stop_gradient`

如果一个函数中有一个部分是不可微的，导致整个函数不可微，可以用`stop_gradient`包裹这个不可微的部分，使得整个函数可以被`jax.grad`处理

## Per-example gradients

有时候，除了计算整个batch的梯度，也有计算单个梯度的需求，Jax也有简单高效的实现，其实就是jit,vmap,grad叠起来

```python
perex_grads = jax.jit(jax.vmap(jax.grad(fn_loss, in_axes=(...,...)))
```

[^1]:https://www.youtube.com/watch?v=wG_nF1awSSY
[^2]:https://jax.readthedocs.io/en/latest/jax-101/04-advanced-autodiff.html#stopping-gradients

