---
title: Jax 101-4 ğŸ¤
subtitle: Advanced Automatic Differentiation in Jax ğŸ‚
date: 2024-01-29 01:20:00
tags: [dl,jax]
series: 5
cover: blog/images/jax.jpg
---

> [!abstract] Abstract
>
> ç®€å•ä½†é‡è¦çš„ä¸€èŠ‚ ğŸ«£

# Advanced Automatic Differentiation in Jax ğŸ‚

## Higher-order derivatives

æ±‚é«˜é˜¶å¯¼æ•°åœ¨Jaxä¸­æ˜¯å®¹æ˜“çš„

ä¾‹å¦‚å¯¹$f(x)=x^3+2x^2-3x+1$ï¼Œå¯ä»¥è¡¨ç¤ºä¸ºï¼š

```python
import jax
f = lambda x: x**3 + 2*x**2 - 3*x + 1
dfdx = jax.grad(f)
```

é‡å¤ä½¿ç”¨`jax.grad`å¾—åˆ°å¤šé˜¶å¯¼æ•°

```python
d2fdx = jax.grad(dfdx)
d3fdx = jax.grad(d2fdx)
d4fdx = jax.grad(d3fdx)
```

æœ‰å¤šä¸ªå‚æ•°æ—¶ï¼Œæ¢¯åº¦å€¼æ˜¯HessiançŸ©é˜µã€‚Jaxæä¾›äº†`jax.jaxfwd`å’Œ`jax.jaxrec`ä¸¤ä¸ªæ–¹æ³•æ¥è®¡ç®—æ¢¯åº¦å€¼ï¼Œåˆ†åˆ«å¯¹åº”forward- and reverse-mode autodiff

> [!note] Note
>
> `jax.jaxfwd`å’Œ`jax.jaxrec`çš„ç»“æœå®Œå…¨ä¸€è‡´ï¼Œåªæ˜¯é€‚ç”¨åœºæ™¯ä¸åŒï¼Œæœ‰æ•ˆç‡åŒºåˆ«
>
> å®˜æ–¹æä¾›çš„è‡ªåŠ¨å¾®åˆ†è§†é¢‘[^1]

> [!info] More information
>
> æ›´å¤šå…³äºæ¢¯åº¦å’ŒhassiançŸ©é˜µçš„ä¼˜åŒ–å’Œä½¿ç”¨æ–¹æ³•åœ¨[The Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)

## Higher order optimization

åœ¨ä¸€äº›metaa-learningæ–¹æ³•ä¸­ï¼ˆå¦‚Model-Agnostic Meta-Learning ([MAML](https://arxiv.org/abs/1703.03400)),ï¼‰éœ€è¦å¯¹æ¢¯åº¦æ›´æ–°è¿‡ç¨‹åšå¾®åˆ†ï¼Œç›¸å½“äºå¯¹ç‰¹å¾å€¼åšå¤šé˜¶å¯¼ï¼Œåœ¨Jaxä¸­å¯ä»¥è½»æ¾å®ç°

```python
def meta_loss_fn(params, data):
  """Computes the loss after one step of SGD."""
  grads = jax.grad(loss_fn)(params, data)
  return loss_fn(params - lr * grads, data)

meta_grads = jax.grad(meta_loss_fn)(params, data)
```

## Stopping gradients

ç”¨`jax.lax.stop_gradient(f)`è¡¨ç¤ºå¿½ç•¥è¿™ä¸€éƒ¨åˆ†çš„æ¢¯åº¦ï¼ˆå‡è£…å…¶ä¸ä¾èµ–äºå‚æ•°xï¼‰

å®˜æ–¹æ–‡æ¡£è¿™é‡Œä¸¾äº†ä¸€ä¸ªRLçš„ä¾‹å­[^2]

### Straight-through estimator using `stop_gradient`

å¦‚æœä¸€ä¸ªå‡½æ•°ä¸­æœ‰ä¸€ä¸ªéƒ¨åˆ†æ˜¯ä¸å¯å¾®çš„ï¼Œå¯¼è‡´æ•´ä¸ªå‡½æ•°ä¸å¯å¾®ï¼Œå¯ä»¥ç”¨`stop_gradient`åŒ…è£¹è¿™ä¸ªä¸å¯å¾®çš„éƒ¨åˆ†ï¼Œä½¿å¾—æ•´ä¸ªå‡½æ•°å¯ä»¥è¢«`jax.grad`å¤„ç†

## Per-example gradients

æœ‰æ—¶å€™ï¼Œé™¤äº†è®¡ç®—æ•´ä¸ªbatchçš„æ¢¯åº¦ï¼Œä¹Ÿæœ‰è®¡ç®—å•ä¸ªæ¢¯åº¦çš„éœ€æ±‚ï¼ŒJaxä¹Ÿæœ‰ç®€å•é«˜æ•ˆçš„å®ç°ï¼Œå…¶å®å°±æ˜¯jit,vmap,gradå èµ·æ¥

```python
perex_grads = jax.jit(jax.vmap(jax.grad(fn_loss, in_axes=(...,...)))
```

[^1]:https://www.youtube.com/watch?v=wG_nF1awSSY
[^2]:https://jax.readthedocs.io/en/latest/jax-101/04-advanced-autodiff.html#stopping-gradients

